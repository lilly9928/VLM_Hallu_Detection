{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2fe759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu')\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from typing import List, Union, Optional, Dict, Tuple\n",
    "import gc\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# import wandb\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, set_seed  # noqa: F401\n",
    "\n",
    "from src.model_zoo import get_model\n",
    "from src.dataset_zoo import get_dataset\n",
    "from src.misc import seed_all, _default_collate, save_scores\n",
    "from src.old.probing_utils_copy import load_llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e16ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkCreate_12_8, version libnvJitLink.so.12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaForConditionalGeneration, Qwen2VLForConditionalGeneration, InstructBlipProcessor, InstructBlipForConditionalGeneration\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(model_name):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/transformers/__init__.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     29\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     logging,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     52\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/transformers/dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/transformers/utils/__init__.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     29\u001b[39m     add_end_docstrings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     replace_return_docstrings,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/transformers/utils/chat_template_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     43\u001b[39m BASIC_TYPES = (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), ...)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/torch/__init__.py:416\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    415\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    420\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: /opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkCreate_12_8, version libnvJitLink.so.12"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaForConditionalGeneration, Qwen2VLForConditionalGeneration, InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "\n",
    "def load_model(model_name):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        cap_major = torch.cuda.get_device_capability(0)[0]  # compute capability of gpu 0\n",
    "        dtype = torch.bfloat16 if cap_major >= 8 else torch.float16\n",
    "        device_map = \"auto\"\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "        device_map = None\n",
    "    \n",
    "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=False,\n",
    "        cache_dir='/data3/hg_weight/hg_weight',\n",
    "        use_fast=False\n",
    "    )\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=device_map,\n",
    "        cache_dir='/data3/hg_weight/hg_weight',\n",
    "    )    \n",
    "    tok = processor.tokenizer\n",
    "    tok.padding_side = \"left\"\n",
    "   \n",
    "    return model, processor, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_all_features_for_batch(attentions: tuple, num_image_tokens: int) -> torch.Tensor:\n",
    "    \n",
    "    # 각 레이어별로 계산된 피처 텐서( (batch, heads, 2) )를 담을 리스트\n",
    "    layer_features_list = []\n",
    "\n",
    "    for layer_attention in attentions:\n",
    "        # layer_attention shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        last_token_attn = layer_attention[:, :, -1, :]\n",
    "        # last_token_attn shape: (batch_size, num_heads, seq_len)\n",
    "        \n",
    "        # --- 피처 1: 어텐션 엔트로피 계산 ---\n",
    "        attention_entropy = -torch.sum(\n",
    "            last_token_attn * torch.log2(last_token_attn + 1e-9),\n",
    "            dim=-1 # 마지막 차원(seq_len)에 대해 합산\n",
    "        )\n",
    "        # attention_entropy shape: (batch_size, num_heads)\n",
    "        \n",
    "        # --- 피처 2: (텍스트 어텐션 합) - (이미지 어텐션 합) 계산 ---\n",
    "        # 이미지 토큰 부분의 어텐션 값 합산\n",
    "        image_attn_sum = torch.sum(last_token_attn[:, :, :num_image_tokens], dim=-1)\n",
    "        \n",
    "        # 텍스트 토큰 부분의 어텐션 값 합산\n",
    "        text_attn_sum = torch.sum(last_token_attn[:, :, num_image_tokens:], dim=-1)\n",
    "        \n",
    "        attention_diff = text_attn_sum - image_attn_sum\n",
    "        # attention_diff shape: (batch_size, num_heads)\n",
    "\n",
    "        # 2개의 피처를 마지막 차원으로 합쳐 (batch_size, num_heads, 2) 모양의 텐서 생성\n",
    "        layer_features = torch.stack([attention_entropy, attention_diff], dim=-1)\n",
    "        layer_features_list.append(layer_features)\n",
    "\n",
    "    # 모든 레이어의 피처 리스트를 쌓아 최종 텐서 생성\n",
    "    # dim=1을 기준으로 쌓아 (batch_size, num_layers, num_heads, 2) 모양을 만듭니다.\n",
    "    final_features_tensor = torch.stack(layer_features_list, dim=1)\n",
    "    \n",
    "    return final_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IS_TEST = True\n",
    "OUTPUT_ROOT = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output\"\n",
    "\n",
    "\n",
    "def generate_with_attention(model, proc, tok, joint_loader, model_type, split):\n",
    "    num_samples = 500\n",
    "    config = model.config\n",
    "    image_size = config.vision_config.image_size\n",
    "    patch_size = config.vision_config.patch_size\n",
    "    \n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    \n",
    "    num_layers = config.vision_config.num_hidden_layers\n",
    "    num_heads = config.vision_config.num_attention_heads\n",
    "    num_features = 2    # entorpy, subtraction\n",
    "    num_image_tokens = num_patches\n",
    "    \n",
    "    all_features_tensor = torch.zeros((num_samples, num_layers, num_heads, num_features))\n",
    "    all_labels_tensor = torch.zeros((num_samples))\n",
    "    \n",
    "    device = model.device\n",
    "\n",
    "    cur_time = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    save_dir  = os.path.join(OUTPUT_ROOT, f\"{model_type}_{DATASET}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"head_conf_{split}_{cur_time}.json\")\n",
    "    features_path = os.path.join(save_dir, 'train_features_500.pt')\n",
    "    \n",
    "    current_idx = 0\n",
    "    batch_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(joint_loader):\n",
    "            \n",
    "            idxs, images, questions, gold_answers, labels, image_paths = batch\n",
    "            batch_size = len(questions)\n",
    "            \n",
    "            prompts = [build_prompt(tok, q, model_type) for q in questions]\n",
    "            images = ensure_images_ok(images)\n",
    "            inputs = proc(\n",
    "                images=images,\n",
    "                text=prompts,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                **inputs,\n",
    "                output_attentions=True\n",
    "            )\n",
    "            attentions = outputs.attentions # (layer1, layer2, layer ...)\n",
    "            features_for_batch = calculate_all_features_for_batch(attentions, num_image_tokens)\n",
    "            \n",
    "            all_features_tensor[current_idx : current_idx + batch_size] = features_for_batch\n",
    "            all_labels_tensor[current_idx : current_idx + batch_size] = batch['labels']\n",
    "            \n",
    "            \n",
    "            batch_cnt += 1\n",
    "            current_idx += batch_size\n",
    "            \n",
    "            if IS_TEST and batch_cnt == 1:\n",
    "                break\n",
    "            \n",
    "    torch.save({'features': all_features_tensor, 'labels': all_labels_tensor}, features_path)\n",
    "    print(f\"All features and labels save with {all_labels_tensor.shape} and feature shape is {all_features_tensor.shape}\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0653681",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m cur_batch = \u001b[32m1\u001b[39m\n\u001b[32m      3\u001b[39m cur_split = \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model, proc, tok = \u001b[43mload_model\u001b[49m(model_type)\n\u001b[32m      6\u001b[39m joint_loader = load_dataset(cur_split, cur_batch)\n\u001b[32m      8\u001b[39m generate_with_attention(model, proc, tok, joint_loader, model_type, cur_split)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_type = \"llava1.5\"\n",
    "cur_batch = 1\n",
    "cur_split = \"train\"\n",
    "\n",
    "model, proc, tok = load_model(model_type)\n",
    "joint_loader = load_dataset(cur_split, cur_batch)\n",
    "\n",
    "generate_with_attention(model, proc, tok, joint_loader, model_type, cur_split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit_hallu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
