{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5692a274",
   "metadata": {},
   "source": [
    "# Proving exp\n",
    "\n",
    "* logit mean, max, min, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88fee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu')\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from typing import List, Union, Optional, Dict, Tuple\n",
    "import gc\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# import wandb\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, set_seed  # noqa: F401\n",
    "\n",
    "from src.model_zoo import get_model\n",
    "from src.dataset_zoo import get_dataset\n",
    "from src.misc import seed_all, _default_collate, save_scores\n",
    "from src.old.probing_utils_copy import load_llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a601bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"VizWiz\"\n",
    "TRAIN_PATH = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/data/preprocess/llava-1.5-7b-hf-vizwiz_train-llava_answers.csv\"\n",
    "VAL_PATH = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/data/preprocess/llava-1.5-7b-hf-vizwiz_val-llava_answers.csv\"\n",
    "SPLIT = \"train\" # or val\n",
    "\n",
    "NUM_WORKERS = 16\n",
    " \n",
    "SEED = 1\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d2c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# image path, question, \n",
    "\n",
    "class Vizwiz(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        start_idx=0,\n",
    "        ):\n",
    "        # image path / question / gold_answer / model_answer / label\n",
    "        # label 0 -> 정답 (no hallucination) / label 1 -> 오답 (hallucination)  \n",
    "        data_cv = pd.read_csv(data_path)\n",
    "        \n",
    "        self.image_paths = data_cv[\"image_path\"].tolist()[start_idx:]\n",
    "        self.questions = data_cv[\"question\"].tolist()[start_idx:]\n",
    "        self.gold_answers = data_cv[\"gold_answer\"].tolist()[start_idx:]\n",
    "        self.hallu_labels = data_cv[\"label\"].tolist()[start_idx:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx: int) :        \n",
    "        image_path = self.image_paths[idx]\n",
    "        question = self.questions[idx]\n",
    "        gold_answer = self.gold_answers[idx]\n",
    "        hallu_label = self.hallu_labels[idx]\n",
    "        \n",
    "        return idx, image_path, question, gold_answer, hallu_label\n",
    "    \n",
    "    \n",
    "    \n",
    "def viz_collate_fn(batch):\n",
    "    idxs, images, questions, gold_answers, labels, image_paths = [], [], [], [], [], []\n",
    "    \n",
    "    for idx, image_path, question, gold_answer, hallu_label in batch:\n",
    "        try:\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new(\"RGB\", (image_size, image_size), (0, 0, 0))\n",
    "        \n",
    "        images.append(img)\n",
    "        questions.append(question)\n",
    "        gold_answers.append(gold_answer)\n",
    "        labels.append(int(hallu_label))\n",
    "        image_paths.append(image_path)\n",
    "        idxs.append(idx)\n",
    "        \n",
    "    return (idxs, images, questions, gold_answers, labels, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a8a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration, Qwen2VLForConditionalGeneration, InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "\n",
    "def load_model(model_name):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        cap_major = torch.cuda.get_device_capability(0)[0]  # compute capability of gpu 0\n",
    "        dtype = torch.bfloat16 if cap_major >= 8 else torch.float16\n",
    "        device_map = \"auto\"\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "        device_map = None\n",
    "    \n",
    "    if \"llava\" in model_name:\n",
    "        model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=False,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "            use_fast=False\n",
    "        )\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=device_map,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "        )    \n",
    "        tok = processor.tokenizer\n",
    "        tok.padding_side = \"left\"\n",
    "        \n",
    "    elif 'qwen' in model_name:\n",
    "        model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "        device_map = \"cuda\"\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=False,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "            use_fast=False\n",
    "        )\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=device_map,\n",
    "            cache_dir=\"/data3/hg_weight/hg_weight\",\n",
    "        )\n",
    "        tok = processor.tokenizer\n",
    "        tok.padding_side = \"left\"\n",
    "        if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "            tok.pad_token = tok.eos_token       \n",
    "        model.generation_config.pad_token_id = tok.pad_token_id\n",
    "        model.generation_config.do_sample = False\n",
    "        # model.generation_config.top_p = 1\n",
    "\n",
    "        \n",
    "    elif 'instructblip' in model_name:\n",
    "        model_id = 'Salesforce/instructblip-vicuna-7b'\n",
    "        processor = InstructBlipProcessor.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=False,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "            use_fast=False\n",
    "        )\n",
    "        model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=device_map,\n",
    "            cache_dir='/data3/hg_weight/hg_weight'\n",
    "        )\n",
    "        tok = processor.tokenizer\n",
    "        tok.padding_side = \"left\"\n",
    "        \n",
    "    else:\n",
    "        print(\"The model should be one of the following: llava1.5-7b, qwen2-vl-7b, instructblip\")\n",
    "        return None\n",
    "    \n",
    "    return model, processor, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36363a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(split, batch_size, start_idx=0):\n",
    "    data_path = TRAIN_PATH if split == 'train' else VAL_PATH\n",
    "    dataset = Vizwiz(data_path, start_idx=start_idx)\n",
    "\n",
    "    joint_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, collate_fn=viz_collate_fn)\n",
    "\n",
    "    return joint_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ebe9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompt\n",
    "def build_prompt(tokenizer, question, model_type) -> str:\n",
    "    if (\"llava\" in model_type):\n",
    "        content = [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question+'Answer in one word.'}]\n",
    "        if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "            messages = [{\"role\": \"user\", \"content\": content}]\n",
    "            try:\n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False\n",
    "                )\n",
    "                return prompt\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return \"<image>\\n\" + question.strip() + \"\\n\"\n",
    "    elif \"qwen\" in model_type:\n",
    "        content = [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question+'Answer in one word.'}]\n",
    "        if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "            # print(\"########## Qwen tokenizer has chat_template attr\")\n",
    "            messages = [{\"role\": \"user\", \"content\": content}]\n",
    "            try:\n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False\n",
    "                )\n",
    "                return prompt\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return \"<image>\\n\" + question.strip() + \"\\n\"\n",
    "    \n",
    "\n",
    "    elif \"instructblip\" in model_type:\n",
    "        prompt = question + \"Answer in one word.\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462ac7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "def fix_tiny_image(img, base=28, round_to_multiple=True):\n",
    "    # img: PIL.Image\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "    else:\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "    w, h = img.size\n",
    "    \n",
    "    if min(w, h) < base:\n",
    "        scale = math.ceil(base / min(w, h))\n",
    "        w, h = w * scale, h * scale\n",
    "        img = img.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "    \n",
    "    if round_to_multiple:\n",
    "        new_w = int(math.ceil(img.width  / base) * base)\n",
    "        new_h = int(math.ceil(img.height / base) * base)\n",
    "        if (new_w, new_h) != img.size:\n",
    "            img = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def ensure_images_ok(images):\n",
    "    fixed = []\n",
    "    for im in images:\n",
    "        if isinstance(im, (str, bytes)):  # 경로일 경우\n",
    "            im = Image.open(im).convert(\"RGB\")\n",
    "        fixed.append(fix_tiny_image(im, base=28, round_to_multiple=True))\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation -> logit mean, max, min, prob 저장\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IS_TEST = False\n",
    "OUTPUT_ROOT = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output\"\n",
    "\n",
    "def run_generation_w_logits(model, proc, tok, joint_loader, model_type, split):\n",
    "    device = model.device\n",
    "    # dtype  = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    cur_time = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    save_dir  = os.path.join(OUTPUT_ROOT, f\"{model_type}_{DATASET}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"logits_{split}_{cur_time}.json\")\n",
    "\n",
    "    all_results = []\n",
    "    batch_cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(joint_loader):\n",
    "            outputs=[]\n",
    "            logit_means, logit_mins, logit_maxs, log_probs = [], [], [], []\n",
    "            \n",
    "            idxs, images, questions, gold_answers, labels, image_paths = batch\n",
    "            prompts = [build_prompt(tok, q, model_type) for q in questions]\n",
    "            images = ensure_images_ok(images)\n",
    "            inputs = proc(\n",
    "                images=images,\n",
    "                text=prompts,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # input_ids = inputs[\"input_ids\"].to(device)\n",
    "            # attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "            # pixel_values = inputs[\"pixel_values\"].to(device=device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,  \n",
    "                max_new_tokens=5,  \n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,   \n",
    "            )\n",
    "            \n",
    "            sequences = outputs.sequences\n",
    "            scores = outputs.scores\n",
    "            logits_0  = scores[0]    \n",
    "            logprobs_0 = F.log_softmax(logits_0, dim=-1)   # (B, V)\n",
    "\n",
    "            batch_size = sequences.size(0)\n",
    "            \n",
    "            start_pos = sequences.size(1) - len(scores)\n",
    "            first_token_ids = sequences[:, start_pos]\n",
    "            \n",
    "            logit_means = logits_0.mean(dim=-1)            # (B,)\n",
    "            logit_mins  = logits_0.min(dim=-1).values      # (B,)\n",
    "            logit_maxs  = logits_0.max(dim=-1).values      # (B,)\n",
    "            \n",
    "            gather_ids       = first_token_ids.unsqueeze(1)             # (B,1)\n",
    "            first_logprobs   = torch.gather(logprobs_0, 1, gather_ids).squeeze(1)  # (B,)\n",
    "            \n",
    "            model_answers = []\n",
    "            \n",
    "            for i in range(batch_size):  \n",
    "                gen_ids = sequences[i, start_pos:] \n",
    "                text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "                model_answers.append(text)\n",
    "                \n",
    "            if IS_TEST:\n",
    "                print(model.generation_config)\n",
    "                i = 3\n",
    "                tid = sequences[:, start_pos][i].item()        # 선택된 첫 토큰 id\n",
    "                lp  = logprobs_0[i, tid].item()                # 해당 토큰 logprob\n",
    "                first_decoded = tok.decode(tid, skip_special_tokens=True)\n",
    "                print(\"stored logprob:\", first_logprobs[i].item(), \"| recomputed:\", lp)\n",
    "\n",
    "                print(\"mean/min/max:\",\n",
    "                    logits_0[i].mean().item(),\n",
    "                    logits_0[i].min().item(),\n",
    "                    logits_0[i].max().item())\n",
    "                \n",
    "                \n",
    "                print(\"saved mean/min/max:\",\n",
    "                    float(logit_means[i].item()),\n",
    "                    float(logit_mins[i].item()),\n",
    "                    float(logit_maxs[i].item())\n",
    "                )\n",
    "\n",
    "                print(\"decoded:\", model_answers[i])\n",
    "                print(\"first_token:\", first_decoded, \"first token id:\", tid)\n",
    "            \n",
    "                \n",
    "            for i in range(batch_size):\n",
    "                rec = {\n",
    "                    \"idx\":             int(idxs[i]),\n",
    "                    \"question\":        questions[i],\n",
    "                    \"image_path\":      image_paths[i],\n",
    "                    \"gold_answer\":     gold_answers[i],\n",
    "                    \"model_answer\":    model_answers[i],\n",
    "                    \"hallu_label\":     labels[i],\n",
    "                    \"logit_mean\":      float(logit_means[i].item()),\n",
    "                    \"logit_min\":       float(logit_mins[i].item()),\n",
    "                    \"logit_max\":       float(logit_maxs[i].item()),\n",
    "                    \"logprob\":         float(first_logprobs[i].item()),\n",
    "                }\n",
    "                all_results.append(rec)\n",
    "            \n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_results, f, ensure_ascii=False, indent=4)\n",
    "                \n",
    "            batch_cnt += 1\n",
    "            if IS_TEST: \n",
    "                if batch_cnt == 1: \n",
    "                    break\n",
    "            \n",
    "        print(f\"Saved {len(all_results)} records to {save_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a5019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run generation llava - val\n",
    "# model_type = \"llava1.5\"\n",
    "# cur_batch = 5\n",
    "# cur_split = \"val\"\n",
    "\n",
    "# model, proc, tok = load_model(model_type)\n",
    "# joint_loader = load_dataset(cur_split, cur_batch)\n",
    "\n",
    "# run_generation_w_logits(model, proc, tok, joint_loader, model_type, cur_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c473dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run generation qwen - val\n",
    "# model_type = \"instructblip\"\n",
    "# cur_batch = 15\n",
    "# cur_split = \"train\"\n",
    "\n",
    "# model, proc, tok = load_model(model_type)\n",
    "# joint_loader = load_dataset(cur_split, cur_batch)\n",
    "\n",
    "# run_generation_w_logits(model, proc, tok, joint_loader, model_type, cur_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:14<00:00,  2.87s/it]\n",
      "  0%|          | 0/745 [00:00<?, ?it/s]/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 745/745 [3:13:38<00:00, 15.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3721 records to /data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/qwen2_VizWiz/logits_train_0918_1358.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# run generation instructblip - val\n",
    "model_type = \"qwen2\"\n",
    "cur_batch = 5\n",
    "cur_split = \"train\"\n",
    "start_idx = 11270\n",
    "\n",
    "model, proc, tok = load_model(model_type)\n",
    "joint_loader = load_dataset(cur_split, cur_batch, start_idx)\n",
    "\n",
    "run_generation_w_logits(model, proc, tok, joint_loader, model_type, cur_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ff65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation and scoring\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGD_TRAIN_PATH = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/logits_0916_2251.json\"\n",
    "\n",
    "with open(SGD_TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "val_data = pd.read_csv(SGD_VAL_PATH)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_eval  = pd.DataFrame(eval_data)\n",
    "\n",
    "# 사용할 단일 특징 목록\n",
    "FEATURES = [\"logit_mean\", \"logit_min\", \"logit_max\", \"logprob\"]\n",
    "TARGET   = \"hallu_label\"\n",
    "\n",
    "y_train_raw = df_train[TARGET].values\n",
    "if np.issubdtype(df_train[TARGET].dtype, np.number):\n",
    "    le = None\n",
    "    y_train = y_train_raw.astype(int)\n",
    "else:\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train_raw)\n",
    "\n",
    "# 평가용 라벨 (동일 인코더 적용)\n",
    "if le is None:\n",
    "    y_eval = df_eval[TARGET].astype(int).values\n",
    "else:\n",
    "    # 평가셋 라벨에 학습셋에 없던 클래스가 있으면 예외 발생하므로 주의\n",
    "    y_eval = le.transform(df_eval[TARGET].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit_hallu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
