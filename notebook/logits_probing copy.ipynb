{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5692a274",
   "metadata": {},
   "source": [
    "# Proving exp\n",
    "\n",
    "* logit mean, max, min, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88fee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/logit_hallu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu')\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from typing import List, Union, Optional, Dict, Tuple\n",
    "import gc\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# import wandb\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, set_seed  # noqa: F401\n",
    "\n",
    "from src.model_zoo import get_model\n",
    "from src.dataset_zoo import get_dataset\n",
    "from src.misc import seed_all, _default_collate, save_scores\n",
    "from src.old.probing_utils_copy import load_llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a601bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"VizWiz\"\n",
    "TRAIN_PATH = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/data/preprocess/llava-1.5-7b-hf-vizwiz_train-llava_answers.csv\"\n",
    "VAL_PATH = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/data/preprocess/llava-1.5-7b-hf-vizwiz_val-llava_answers.csv\"\n",
    "SPLIT = \"train\" # or val\n",
    "\n",
    "NUM_WORKERS = 16\n",
    " \n",
    "SEED = 1\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40d2c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# image path, question, \n",
    "\n",
    "class Vizwiz(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        subset_size=500,\n",
    "        start_idx=0,\n",
    "        ):\n",
    "        # image path / question / gold_answer / model_answer / label\n",
    "        # label 0 -> 정답 (no hallucination) / label 1 -> 오답 (hallucination)  \n",
    "        data_cv = pd.read_csv(data_path)\n",
    "        if start_idx > 0:\n",
    "            self.image_paths = data_cv[\"image_path\"].tolist()[start_idx:]\n",
    "            self.questions = data_cv[\"question\"].tolist()[start_idx:]\n",
    "            self.gold_answers = data_cv[\"gold_answer\"].tolist()[start_idx:]\n",
    "            self.hallu_labels = data_cv[\"label\"].tolist()[start_idx:]\n",
    "        else:\n",
    "            if subset_size > 0:\n",
    "                subset_indices = np.random.choice(len(data_cv), subset_size, replace=False)\n",
    "                final_data = data_cv.iloc[subset_indices].reset_index(drop=True)\n",
    "            else:\n",
    "                final_data = data_cv\n",
    "            \n",
    "            self.image_paths = final_data[\"image_path\"].tolist()\n",
    "            self.questions = final_data[\"question\"].tolist()\n",
    "            self.gold_answers = final_data[\"gold_answer\"].tolist()\n",
    "            self.hallu_labels = final_data[\"label\"].tolist()\n",
    "        \n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx: int) :        \n",
    "        image_path = self.image_paths[idx]\n",
    "        question = self.questions[idx]\n",
    "        gold_answer = self.gold_answers[idx]\n",
    "        hallu_label = self.hallu_labels[idx]\n",
    "        \n",
    "        return idx, image_path, question, gold_answer, hallu_label\n",
    "    \n",
    "    \n",
    "    \n",
    "def viz_collate_fn(batch):\n",
    "    idxs, images, questions, gold_answers, labels, image_paths = [], [], [], [], [], []\n",
    "    \n",
    "    for idx, image_path, question, gold_answer, hallu_label in batch:\n",
    "        try:\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new(\"RGB\", (image_size, image_size), (0, 0, 0))\n",
    "        \n",
    "        images.append(img)\n",
    "        questions.append(question)\n",
    "        gold_answers.append(gold_answer)\n",
    "        labels.append(int(hallu_label))\n",
    "        image_paths.append(image_path)\n",
    "        idxs.append(idx)\n",
    "        \n",
    "    return (idxs, images, questions, gold_answers, labels, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a8a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration, Qwen2VLForConditionalGeneration, InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "\n",
    "def load_model(model_name):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        cap_major = torch.cuda.get_device_capability(0)[0]  # compute capability of gpu 0\n",
    "        dtype = torch.bfloat16 if cap_major >= 8 else torch.float16\n",
    "        device_map = \"auto\"\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "        device_map = None\n",
    "    \n",
    "    if \"llava\" in model_name:\n",
    "        model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=False,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "            use_fast=False\n",
    "        )\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=device_map,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "        )    \n",
    "        tok = processor.tokenizer\n",
    "        tok.padding_side = \"left\"\n",
    "        \n",
    "    elif 'qwen' in model_name:\n",
    "        model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "        device_map = \"cuda\"\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=False,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "            use_fast=False\n",
    "        )\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=device_map,\n",
    "            cache_dir=\"/data3/hg_weight/hg_weight\",\n",
    "        )\n",
    "        tok = processor.tokenizer\n",
    "        tok.padding_side = \"left\"\n",
    "        if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "            tok.pad_token = tok.eos_token       \n",
    "        model.generation_config.pad_token_id = tok.pad_token_id\n",
    "        model.generation_config.do_sample = False\n",
    "        # model.generation_config.top_p = 1\n",
    "\n",
    "        \n",
    "    elif 'instructblip' in model_name:\n",
    "        model_id = 'Salesforce/instructblip-vicuna-7b'\n",
    "        processor = InstructBlipProcessor.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=False,\n",
    "            cache_dir='/data3/hg_weight/hg_weight',\n",
    "            use_fast=False\n",
    "        )\n",
    "        model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=device_map,\n",
    "            cache_dir='/data3/hg_weight/hg_weight'\n",
    "        )\n",
    "        tok = processor.tokenizer\n",
    "        tok.padding_side = \"left\"\n",
    "        \n",
    "    else:\n",
    "        print(\"The model should be one of the following: llava1.5-7b, qwen2-vl-7b, instructblip\")\n",
    "        return None\n",
    "    \n",
    "    return model, processor, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ac7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_layers_from_config(model) -> int:\n",
    "    cfg = getattr(model, \"config\", None)\n",
    "    if hasattr(cfg, \"text_config\") and hasattr(cfg.text_config, \"num_hidden_layers\"):\n",
    "        return int(cfg.text_config.num_hidden_layers)\n",
    "    if hasattr(cfg, \"num_hidden_layers\"):\n",
    "        return int(cfg.num_hidden_layers)\n",
    "    raise ValueError(\"Cannot find num_hidden_layers in config.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36363a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(split, batch_size, subset_size = 500, start_idx=0):\n",
    "    data_path = TRAIN_PATH if split == 'train' else VAL_PATH\n",
    "    dataset = Vizwiz(data_path, subset_size=subset_size, start_idx=start_idx)\n",
    "\n",
    "    joint_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, collate_fn=viz_collate_fn)\n",
    "\n",
    "    return joint_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ebe9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompt\n",
    "def build_prompt(tokenizer, question, model_type) -> str:\n",
    "    if (\"llava\" in model_type):\n",
    "        content = [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question+'Answer in one word.'}]\n",
    "        if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "            messages = [{\"role\": \"user\", \"content\": content}]\n",
    "            try:\n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False\n",
    "                )\n",
    "                return prompt\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return \"<image>\\n\" + question.strip() + \"\\n\"\n",
    "    elif \"qwen\" in model_type:\n",
    "        content = [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question+'Answer in one word.'}]\n",
    "        if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "            # print(\"########## Qwen tokenizer has chat_template attr\")\n",
    "            messages = [{\"role\": \"user\", \"content\": content}]\n",
    "            try:\n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False\n",
    "                )\n",
    "                return prompt\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return \"<image>\\n\" + question.strip() + \"\\n\"\n",
    "    \n",
    "\n",
    "    elif \"instructblip\" in model_type:\n",
    "        prompt = question + \"Answer in one word.\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462ac7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "def fix_tiny_image(img, base=28, round_to_multiple=True):\n",
    "    # img: PIL.Image\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "    else:\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "    w, h = img.size\n",
    "    \n",
    "    if min(w, h) < base:\n",
    "        scale = math.ceil(base / min(w, h))\n",
    "        w, h = w * scale, h * scale\n",
    "        img = img.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "    \n",
    "    if round_to_multiple:\n",
    "        new_w = int(math.ceil(img.width  / base) * base)\n",
    "        new_h = int(math.ceil(img.height / base) * base)\n",
    "        if (new_w, new_h) != img.size:\n",
    "            img = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def ensure_images_ok(images):\n",
    "    fixed = []\n",
    "    for im in images:\n",
    "        if isinstance(im, (str, bytes)):  # 경로일 경우\n",
    "            im = Image.open(im).convert(\"RGB\")\n",
    "        fixed.append(fix_tiny_image(im, base=28, round_to_multiple=True))\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2121a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IS_TEST = False\n",
    "OUTPUT_ROOT = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output\"\n",
    "\n",
    "def attention_entropy(attn_weights):\n",
    "    \"\"\"\n",
    "    attn_weights: (B, num_heads, tgt_len, src_len)\n",
    "    return: (B, num_heads) 평균 entropy (tgt_len 평균)\n",
    "    \"\"\"\n",
    "    # entropy = -sum(p log p)\n",
    "    ent = -(attn_weights * torch.clamp(attn_weights, min=1e-9).log()).sum(dim=-1)  # (B, H, tgt_len)\n",
    "    ent = ent.mean(dim=-1)  # 평균 over tgt_len\n",
    "    return ent  # (B, H)\n",
    "\n",
    "def run_generation_w_logits(model, proc, tok, joint_loader, model_type, split):\n",
    "    device = model.device\n",
    "\n",
    "    cur_time = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    save_dir  = os.path.join(OUTPUT_ROOT, f\"{model_type}_{DATASET}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"logits_{split}_{cur_time}.json\")\n",
    "\n",
    "    all_results = []\n",
    "    batch_cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(joint_loader):\n",
    "            idxs, images, questions, gold_answers, labels, image_paths = batch\n",
    "            prompts = [build_prompt(tok, q, model_type) for q in questions]\n",
    "            images = ensure_images_ok(images)\n",
    "            inputs = proc(\n",
    "                images=images,\n",
    "                text=prompts,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            # 1) 먼저 logits/attention 뽑기\n",
    "            outputs = model(\n",
    "                **inputs,\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # logits (마지막 layer)\n",
    "            logits = outputs.logits[:, -1, :]   # (B, V)\n",
    "            logprobs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            # attention: list of layer outputs\n",
    "            # 각 layer: (B, num_heads, tgt_len, src_len)\n",
    "            attentions = outputs.attentions\n",
    "\n",
    "            # head별 entropy 계산 (마지막 layer만 예시로)\n",
    "            attn_last = attentions[-1]   # (B, H, tgt_len, src_len)\n",
    "            entropies = attention_entropy(attn_last)  # (B, H)\n",
    "\n",
    "            # 모델 generation 호출 (텍스트 생성)\n",
    "            gen_out = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,  \n",
    "                max_new_tokens=5,  \n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "            sequences = gen_out.sequences\n",
    "            scores = gen_out.scores\n",
    "            logits_0  = scores[0]    \n",
    "            logprobs_0 = F.log_softmax(logits_0, dim=-1)   # (B, V)\n",
    "\n",
    "            batch_size = sequences.size(0)\n",
    "            start_pos = sequences.size(1) - len(scores)\n",
    "            first_token_ids = sequences[:, start_pos]\n",
    "\n",
    "            logit_means = logits_0.mean(dim=-1)            \n",
    "            logit_mins  = logits_0.min(dim=-1).values      \n",
    "            logit_maxs  = logits_0.max(dim=-1).values      \n",
    "            \n",
    "            gather_ids       = first_token_ids.unsqueeze(1)             \n",
    "            first_logprobs   = torch.gather(logprobs_0, 1, gather_ids).squeeze(1)  \n",
    "\n",
    "            model_answers = []\n",
    "            for i in range(batch_size):  \n",
    "                gen_ids = sequences[i, start_pos:] \n",
    "                text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "                model_answers.append(text)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                rec = {\n",
    "                    \"idx\":             int(idxs[i]),\n",
    "                    \"question\":        questions[i],\n",
    "                    \"image_path\":      image_paths[i],\n",
    "                    \"gold_answer\":     gold_answers[i],\n",
    "                    \"model_answer\":    model_answers[i],\n",
    "                    \"hallu_label\":     labels[i],\n",
    "                    \"logit_mean\":      float(logit_means[i].item()),\n",
    "                    \"logit_min\":       float(logit_mins[i].item()),\n",
    "                    \"logit_max\":       float(logit_maxs[i].item()),\n",
    "                    \"logprob\":         float(first_logprobs[i].item()),\n",
    "                    # head별 attention entropy 저장\n",
    "                    \"attn_entropy\":    entropies[i].tolist()  \n",
    "                }\n",
    "                all_results.append(rec)\n",
    "\n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_results, f, ensure_ascii=False, indent=4)\n",
    "                \n",
    "            batch_cnt += 1\n",
    "            if IS_TEST and batch_cnt == 1: \n",
    "                break\n",
    "            \n",
    "        print(f\"Saved {len(all_results)} records to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebc9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_all_features_for_batch(attentions: tuple, num_image_tokens: int) -> torch.Tensor:\n",
    "    \n",
    "    # 각 레이어별로 계산된 피처 텐서( (batch, heads, 2) )를 담을 리스트\n",
    "    layer_features_list = []\n",
    "\n",
    "    for layer_attention in attentions:\n",
    "        # layer_attention shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        last_token_attn = layer_attention[:, :, -1, :]\n",
    "        # last_token_attn shape: (batch_size, num_heads, seq_len)\n",
    "        \n",
    "        # --- 피처 1: 어텐션 엔트로피 계산 ---\n",
    "        attention_entropy = -torch.sum(\n",
    "            last_token_attn * torch.log2(last_token_attn + 1e-9),\n",
    "            dim=-1 # 마지막 차원(seq_len)에 대해 합산\n",
    "        )\n",
    "        # attention_entropy shape: (batch_size, num_heads)\n",
    "        \n",
    "        # --- 피처 2: (텍스트 어텐션 합) - (이미지 어텐션 합) 계산 ---\n",
    "        # 이미지 토큰 부분의 어텐션 값 합산\n",
    "        image_attn_sum = torch.sum(last_token_attn[:, :, :num_image_tokens], dim=-1)\n",
    "        \n",
    "        # 텍스트 토큰 부분의 어텐션 값 합산\n",
    "        text_attn_sum = torch.sum(last_token_attn[:, :, num_image_tokens:], dim=-1)\n",
    "        \n",
    "        attention_diff = text_attn_sum - image_attn_sum\n",
    "        # attention_diff shape: (batch_size, num_heads)\n",
    "\n",
    "        # 2개의 피처를 마지막 차원으로 합쳐 (batch_size, num_heads, 2) 모양의 텐서 생성\n",
    "        layer_features = torch.stack([attention_entropy, attention_diff], dim=-1)\n",
    "        layer_features_list.append(layer_features)\n",
    "\n",
    "    # 모든 레이어의 피처 리스트를 쌓아 최종 텐서 생성\n",
    "    # dim=1을 기준으로 쌓아 (batch_size, num_layers, num_heads, 2) 모양을 만듭니다.\n",
    "    final_features_tensor = torch.stack(layer_features_list, dim=1)\n",
    "    \n",
    "    return final_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1706e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IS_TEST = False\n",
    "OUTPUT_ROOT = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output\"\n",
    "\n",
    "\n",
    "def generate_with_attention(model, proc, tok, joint_loader, model_type, split, subset_size):\n",
    "    num_samples = subset_size\n",
    "    config = model.config\n",
    "    image_size = config.vision_config.image_size\n",
    "    patch_size = config.vision_config.patch_size\n",
    "    \n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    \n",
    "    num_layers = model.config.text_config.num_hidden_layers  # -> 32\n",
    "    num_heads = model.config.text_config.num_attention_heads   # -> 32\n",
    "    num_features = 2    # entorpy, subtraction\n",
    "    num_image_tokens = num_patches\n",
    "    \n",
    "    all_features_tensor = torch.zeros((num_samples, num_layers, num_heads, num_features))\n",
    "    all_labels_tensor = torch.zeros((num_samples))\n",
    "    \n",
    "    device = model.device\n",
    "\n",
    "    cur_time = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    save_dir  = os.path.join(OUTPUT_ROOT, f\"{model_type}_{DATASET}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"head_conf_{split}_{cur_time}.json\")\n",
    "    features_path = os.path.join(save_dir, f'train_features_{subset_size}.pt')\n",
    "    \n",
    "    current_idx = 0\n",
    "    batch_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(joint_loader):\n",
    "            \n",
    "            idxs, images, questions, gold_answers, labels, image_paths = batch\n",
    "            \n",
    "  \n",
    "            batch_size = len(questions)\n",
    "            \n",
    "            prompts = [build_prompt(tok, q, model_type) for q in questions]\n",
    "            images = ensure_images_ok(images)\n",
    "            inputs = proc(\n",
    "                images=images,\n",
    "                text=prompts,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                **inputs,\n",
    "                output_attentions=True\n",
    "            )\n",
    "            attentions = outputs.attentions # (layer1, layer2, layer ...)\n",
    "            # vision_attentions = outputs.encoder_attentions\n",
    "            features_for_batch = calculate_all_features_for_batch(attentions, num_image_tokens)\n",
    "            \n",
    "            all_features_tensor[current_idx : current_idx + batch_size] = features_for_batch\n",
    "            all_labels_tensor[current_idx : current_idx + batch_size] = torch.tensor(labels, dtype=torch.float)\n",
    "            \n",
    "            \n",
    "            batch_cnt += 1\n",
    "            current_idx += batch_size\n",
    "            \n",
    "            if IS_TEST and batch_cnt == 1:\n",
    "                break\n",
    "            \n",
    "    torch.save({'features': all_features_tensor, 'labels': all_labels_tensor}, features_path)\n",
    "    print(f\"All features and labels save with {all_labels_tensor.shape} and feature shape is {all_features_tensor.shape}\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a1604",
   "metadata": {},
   "source": [
    "# image token atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "662d0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_patch_len_from_inputs_and_model(inputs, model):\n",
    "    # pixel_values: (B, 3, H, W)\n",
    "    H, W = inputs[\"pixel_values\"].shape[-2:]\n",
    "    try:\n",
    "        patch = getattr(model.get_vision_tower(), \"vision_tower\", model.get_vision_tower()).config.patch_size\n",
    "    except Exception:\n",
    "        patch = getattr(model.config.vision_config, \"patch_size\", 14)\n",
    "    V = (H // patch) * (W // patch)\n",
    "    return V\n",
    "\n",
    "def _build_text_image_key_masks(tokenizer, inputs, attentions, model):\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      text_mask, image_mask: (B, K)  — K는 확장 후 키 길이(attentions[0].size(-1))\n",
    "    \"\"\"\n",
    "    input_ids = inputs['input_ids']\n",
    "    print(f\"inputs: {input_ids.shape}\")\n",
    "    print(f\"attentions: {len(attentions)}\")\n",
    "    print(f\"attention shape: {attentions[0].shape}\")\n",
    "    device = inputs[\"input_ids\"].device\n",
    "    B = inputs[\"input_ids\"].size(0)\n",
    "    K = attentions[0].size(-1)\n",
    "    V = _get_patch_len_from_inputs_and_model(inputs, model)\n",
    "\n",
    "    image_token_id = tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "    input_ids = inputs[\"input_ids\"]  # (B, L0)\n",
    "\n",
    "    image_masks = []\n",
    "    text_masks  = []\n",
    "    for b in range(B):\n",
    "        ids = input_ids[b]\n",
    "        image_pos = (ids == image_token_id).nonzero(as_tuple=False).flatten().tolist()\n",
    "\n",
    "        key_mask_image = torch.zeros(K, dtype=torch.bool, device=device)\n",
    "        shift = 0\n",
    "        for p in image_pos:\n",
    "            start = p + shift\n",
    "            end   = min(start + V, K)\n",
    "            if start < K:\n",
    "                key_mask_image[start:end] = True\n",
    "            shift += (V - 1)\n",
    "        key_mask_text = ~key_mask_image\n",
    "        image_masks.append(key_mask_image)\n",
    "        text_masks.append(key_mask_text)\n",
    "\n",
    "    image_masks = torch.stack(image_masks, dim=0)  # (B, K)\n",
    "    text_masks  = torch.stack(text_masks,  dim=0)  # (B, K)\n",
    "    return text_masks, image_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b34584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def calculate_attn_sum(\n",
    "    attentions: tuple,\n",
    "    num_image_tokens: int,\n",
    "    last_token_idx_or_vec,\n",
    "    text_key_mask: torch.Tensor = None,   # (B, K) optional\n",
    "    image_key_mask: torch.Tensor = None   # (B, K) optional\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    반환: (B, num_layers, num_heads, 2) — [..., 0]=image_sum, [..., 1]=text_sum\n",
    "    - 기존 시그니처 유지하면서 last_token_idx를 *벡터(B,)*도 허용\n",
    "    - 마스크가 주어지면 정확한 위치로 합산, 없으면 기존 슬라이싱 방식으로 fallback\n",
    "    \"\"\"\n",
    "    layer_features_list = []\n",
    "\n",
    "    # last_token_idx 처리: scalar -> (B,), already (B,)면 그대로\n",
    "    if isinstance(last_token_idx_or_vec, torch.Tensor):\n",
    "        last_idx_vec = last_token_idx_or_vec\n",
    "    else:\n",
    "        # scalar라면 B 추정: attentions[0] shape로 K/Q 확인\n",
    "        B = attentions[0].size(0)\n",
    "        last_idx_vec = torch.full((B,), int(last_token_idx_or_vec), dtype=torch.long, device=attentions[0].device)\n",
    "\n",
    "    use_masks = (text_key_mask is not None) and (image_key_mask is not None)\n",
    "\n",
    "    for layer_attention in attentions:\n",
    "        # (B, H, Q, K)\n",
    "        B, H, Q, K = layer_attention.shape\n",
    "\n",
    "        # 배치별 마지막 쿼리 한 줄씩 뽑기: (B, H, K)\n",
    "        idx = last_idx_vec.to(layer_attention.device).view(B, 1, 1, 1).expand(B, H, 1, K)  # (B,H,1,K)\n",
    "        last_token_attn = torch.gather(layer_attention, dim=2, index=idx).squeeze(2)  \n",
    "\n",
    "        if use_masks:\n",
    "            # (B,H,K) * (B,1,K) 브로드캐스트 위해 unsqueeze\n",
    "            image_sum = (last_token_attn * image_key_mask.unsqueeze(1)).sum(dim=-1)  # (B,H)\n",
    "            text_sum  = (last_token_attn * text_key_mask.unsqueeze(1)).sum(dim=-1)   # (B,H)\n",
    "        else:\n",
    "            # === 기존 로직과 최대한 동일한 fallback ===\n",
    "            # 이미지 토큰이 항상 앞쪽에 연속으로 존재한다고 가정(권장 X)\n",
    "            image_sum = torch.sum(last_token_attn[:, :, :num_image_tokens], dim=-1)\n",
    "            text_sum  = torch.sum(last_token_attn[:, :, num_image_tokens:], dim=-1)\n",
    "\n",
    "        layer_features = torch.stack([image_sum, text_sum], dim=-1)  # (B, H, 2)\n",
    "        layer_features_list.append(layer_features)\n",
    "\n",
    "    final_features_tensor = torch.stack(layer_features_list, dim=1)  # (B, L, H, 2)\n",
    "    return final_features_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IS_TEST = True\n",
    "OUTPUT_ROOT = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output\"\n",
    "\n",
    "# 모든 head별 token, image attention 합 구하기\n",
    "def generate_with_image_text(model, proc, tok, joint_loader, model_type, split, target_label):\n",
    "    \n",
    "    is_hall  = False\n",
    "    config = model.config\n",
    "    image_size = config.vision_config.image_size\n",
    "    patch_size = config.vision_config.patch_size\n",
    "    \n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    \n",
    "    num_layers = model.config.text_config.num_hidden_layers  # -> 32\n",
    "    num_heads = model.config.text_config.num_attention_heads   # -> 32\n",
    "    num_image_tokens = num_patches\n",
    "    print(f\"Num image tokens: {num_image_tokens}\")\n",
    "    \n",
    "    \n",
    "    device = model.device\n",
    "\n",
    "    cur_time = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    save_dir  = os.path.join(OUTPUT_ROOT, f\"{model_type}_{DATASET}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"head_w_txt_img_{target_label}_{split}_{cur_time}.json\")\n",
    "    \n",
    "    current_idx = 0\n",
    "    batch_cnt = 0\n",
    "    \n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(joint_loader):\n",
    "                \n",
    "                idxs, images, questions, gold_answers, labels, image_paths = batch\n",
    "                batch_size = len(questions)\n",
    "                cur_lable = labels[0]\n",
    "                if cur_lable != target_label:\n",
    "                    continue\n",
    "                \n",
    "                prompts = [build_prompt(tok, q, model_type) for q in questions]\n",
    "                images = ensure_images_ok(images)\n",
    "                inputs = proc(\n",
    "                    images=images,\n",
    "                    text=prompts,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                \n",
    "                attention_mask = inputs['attention_mask']\n",
    "                print(f\"atten_shape: {attention_mask.shape}\")\n",
    "                last_token_idx_vec = attention_mask.sum(dim=1) - 1  # (B,)\n",
    "\n",
    "                \n",
    "                outputs = model(\n",
    "                    **inputs,\n",
    "                    output_attentions=True\n",
    "                )\n",
    "                attentions = outputs.attentions # (layer1, layer2, layer ...) -> LLM 레이어 수 tuple\n",
    "                # 각 tupe의 shape -> (Batch, Head, Qeury_token_num, Key/value_token_num)\n",
    "                # Q = K = L_exp -> 전체(확장된) 시퀀스의 atention\n",
    "                # 여기서 q(각 토큰 위치)의 softmax(scores)가 들어있음 \n",
    "                text_key_mask, image_key_mask = _build_text_image_key_masks(tok, inputs, attentions, model)  # (B,K)\n",
    "\n",
    "                sums_tensor_batch  = calculate_attn_sum(\n",
    "                    attentions,\n",
    "                    num_image_tokens=num_image_tokens,                 # fallback용 그대로 유지\n",
    "                    last_token_idx_or_vec=last_token_idx_vec,          # (B,)\n",
    "                    text_key_mask=text_key_mask,\n",
    "                    image_key_mask=image_key_mask\n",
    "                )  # (B, L, H, 2)\n",
    "\n",
    "                \n",
    "                for b in range(batch_size):\n",
    "                    for layer_idx in range(num_layers):\n",
    "                        for head_idx in range(num_heads):\n",
    "                            image_sum = sums_tensor_batch[b, layer_idx, head_idx, 0].item()\n",
    "                            text_sum  = sums_tensor_batch[b, layer_idx, head_idx, 1].item()\n",
    "                            json_line = {\n",
    "                                \"idx\": idxs[b],\n",
    "                                \"question\": questions[b],\n",
    "                                \"layer\": layer_idx,\n",
    "                                \"head\": head_idx,\n",
    "                                \"image_attention_sum\": image_sum,\n",
    "                                \"text_attention_sum\": text_sum,\n",
    "                                \"text_sub_image\" : float(text_sum-image_sum)\n",
    "                            }\n",
    "                            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "                \n",
    "                batch_cnt += 1\n",
    "            \n",
    "                current_idx += batch_size\n",
    "                if IS_TEST and batch_cnt == 1:\n",
    "                    break\n",
    "                \n",
    "                # if batch_cnt == 5:\n",
    "                #     break    \n",
    "                \n",
    "            \n",
    "        print(f\"\\n✅ Analysis finished. Results saved to: {save_path}\")\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac47ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num image tokens: 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/14891 [00:05<22:38:11,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atten_shape: torch.Size([1, 600])\n",
      "inputs: torch.Size([1, 600])\n",
      "attentions: 32\n",
      "attention shape: torch.Size([1, 32, 600, 600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/14891 [00:06<26:00:04,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Analysis finished. Results saved to: /data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/head_w_txt_img_1_train_0924_2015.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"llava1.5\"\n",
    "cur_batch = 1\n",
    "cur_split = \"train\"\n",
    "# subset_size = 100\n",
    "start_idx = 100\n",
    "\n",
    "model, proc, tok = load_model(model_type)\n",
    "joint_loader = load_dataset(cur_split, cur_batch, start_idx=start_idx)\n",
    "\n",
    "generate_with_image_text(model, proc, tok, joint_loader, model_type, cur_split, target_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallu len: 5120\n",
      "non-hallu len: 5120\n",
      "Avg diff on hallucination samples:\n",
      "[hall] item 0 avg diff: 0.17899491311982274\n",
      "Avg diff on hallucination samples:\n",
      "[hall] item 1 avg diff: 0.2964126104488969\n",
      "Avg diff on hallucination samples:\n",
      "[hall] item 2 avg diff: 0.37307063676416874\n",
      "Avg diff on hallucination samples:\n",
      "[hall] item 3 avg diff: 0.36741162091493607\n",
      "Avg diff on hallucination samples:\n",
      "[hall] item 4 avg diff: 0.3108724243938923\n",
      "Avg diff on non-hallucination samples:\n",
      "[non-hall] item 0 avg diff: 0.22838062653318048\n",
      "Avg diff on non-hallucination samples:\n",
      "[non-hall] item 1 avg diff: 0.3602502578869462\n",
      "Avg diff on non-hallucination samples:\n",
      "[non-hall] item 2 avg diff: 0.26232808269560337\n",
      "Avg diff on non-hallucination samples:\n",
      "[non-hall] item 3 avg diff: 0.23542529810220003\n",
      "Avg diff on non-hallucination samples:\n",
      "[non-hall] item 4 avg diff: 0.29833515733480453\n"
     ]
    }
   ],
   "source": [
    "hall_path = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/head_w_txt_img_1_train_0924_1907.json\"\n",
    "non_hall_path = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/head_w_txt_img_0_train_0924_1905.json\"\n",
    "\n",
    "head_cnt = 1024\n",
    "item_cnt = 5\n",
    "\n",
    "hall_data = []\n",
    "non_hall_data = []\n",
    "with open(hall_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        hall_data.append(json.loads(line))\n",
    "\n",
    "print(f\"hallu len: {len(hall_data)}\")       \n",
    "\n",
    "with open(non_hall_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        non_hall_data.append(json.loads(line))\n",
    "print(f\"non-hallu len: {len(non_hall_data)}\")       \n",
    "\n",
    "\n",
    "diffs_label_0 = [[] for _ in range(item_cnt)]\n",
    "diffs_label_1 = [[] for _ in range(item_cnt)]\n",
    "\n",
    "cur_item = 0\n",
    "for item in hall_data:\n",
    "    if cur_item >= item_cnt:\n",
    "        break\n",
    "    \n",
    "    difference = item['text_attention_sum'] - item['image_attention_sum']\n",
    "    diffs_label_1[cur_item].append(difference)\n",
    "    \n",
    "    if len(diffs_label_1[cur_item]) == head_cnt:\n",
    "        cur_item += 1\n",
    "    \n",
    "cur_item = 0\n",
    "for item in non_hall_data:\n",
    "    if cur_item >= item_cnt:\n",
    "        break\n",
    "    \n",
    "    difference = item['text_attention_sum'] - item['image_attention_sum']\n",
    "    diffs_label_0[cur_item].append(difference)\n",
    "    \n",
    "    if len(diffs_label_0[cur_item]) == head_cnt:\n",
    "        cur_item += 1\n",
    "\n",
    "for i in range(item_cnt): \n",
    "    print(\"Avg diff on hallucination samples:\")\n",
    "    print(f\"[hall] item {i} avg diff:\", np.mean(diffs_label_1[i]))\n",
    "    \n",
    "for i in range(item_cnt): \n",
    "    print(\"Avg diff on non-hallucination samples:\")\n",
    "    print(f\"[non-hall] item {i} avg diff:\", np.mean(diffs_label_0[i]))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb335608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallu len: 1024\n",
      "non-hallu len: 1024\n",
      "========== Hallucination samples: ======================\n",
      "Max diff: 0.97613525390625\n",
      "Min diff: -0.9998898506164551\n",
      "Avg diff: 0.28105138381943107\n",
      "Number of heads beyond threshod: 547\n",
      "========== Non-hallucination samples: ======================\n",
      "Max diff: 0.97576904296875\n",
      "Min diff: -0.9998712539672852\n",
      "Avg diff: 0.23986634891480207\n",
      "Number of heads beyond threshod: 510\n"
     ]
    }
   ],
   "source": [
    "hall_path = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/head_w_txt_img_1_train_0924_1949.json\"\n",
    "non_hall_path = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/head_w_txt_img_0_train_0924_1948.json\"\n",
    "\n",
    "hall_data = []\n",
    "non_hall_data = []\n",
    "THRESHOLD = 0.45\n",
    "\n",
    "with open(hall_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        hall_data.append(json.loads(line))\n",
    "\n",
    "print(f\"hallu len: {len(hall_data)}\")       \n",
    "\n",
    "with open(non_hall_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        non_hall_data.append(json.loads(line))\n",
    "print(f\"non-hallu len: {len(non_hall_data)}\")       \n",
    "\n",
    "\n",
    "diffs_label_0 = []\n",
    "diffs_label_0_w_th = []\n",
    "diffs_label_1 = []\n",
    "diffs_label_1_w_th = []\n",
    "\n",
    "for hall, non_hall in zip(hall_data, non_hall_data):\n",
    "    hall_difference = hall['text_sub_image']\n",
    "    non_hall_difference = non_hall['text_sub_image']\n",
    "    \n",
    "    diffs_label_1.append(difference)\n",
    "    if difference >= THRESHOLD:\n",
    "        new_data = {\n",
    "            \"layer\": item['layer'],\n",
    "            \"head\": item['head'],\n",
    "            \"hallu_diff\": difference \n",
    "        }\n",
    "        diffs_label_1_w_th.append(difference)\n",
    "\n",
    "    \n",
    "for item in non_hall_data:\n",
    "    difference = item['text_sub_image']\n",
    "    diffs_label_0.append(difference)\n",
    "    if difference >= THRESHOLD:\n",
    "        diffs_label_0_w_th.append(difference)\n",
    "\n",
    "\n",
    "print(\"========== Hallucination samples: ======================\")\n",
    "print(f\"Max diff: {np.max(diffs_label_1)}\")\n",
    "print(f\"Min diff: {np.min(diffs_label_1)}\")\n",
    "print(f\"Avg diff: {np.mean(diffs_label_1)}\")\n",
    "\n",
    "print(f\"Number of heads beyond threshod: {len(diffs_label_1_w_th)}\")\n",
    "\n",
    " \n",
    "print(\"========== Non-hallucination samples: ======================\")\n",
    "print(f\"Max diff: {np.max(diffs_label_0)}\")\n",
    "print(f\"Min diff: {np.min(diffs_label_0)}\")\n",
    "print(f\"Avg diff: {np.mean(diffs_label_0)}\")\n",
    "\n",
    "print(f\"Number of heads beyond threshod: {len(diffs_label_0_w_th)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db3b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "100%|██████████| 1000/1000 [13:21<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels save with torch.Size([5000]) and feature shape is torch.Size([5000, 32, 32, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"llava1.5\"\n",
    "cur_batch = 5\n",
    "cur_split = \"train\"\n",
    "\n",
    "model, proc, tok = load_model(model_type)\n",
    "joint_loader = load_dataset(cur_split, cur_batch, subset_size=5000)\n",
    "\n",
    "generate_with_attention(model, proc, tok, joint_loader, model_type, cur_split, subset_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9384c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]\n",
      "100%|██████████| 200/200 [02:28<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels save with torch.Size([1000]) and feature shape is torch.Size([1000, 32, 32, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"llava1.5\"\n",
    "cur_batch = 5\n",
    "cur_split = \"val\"\n",
    "\n",
    "model, proc, tok = load_model(model_type)\n",
    "joint_loader = load_dataset(cur_split, cur_batch, subset_size=1000)\n",
    "\n",
    "generate_with_attention(model, proc, tok, joint_loader, model_type, cur_split, subset_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe133306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불러온 피처 텐서의 모양: torch.Size([500, 32, 32, 2])\n",
      "불러온 라벨 텐서의 모양: torch.Size([500])\n",
      "tensor(0.4160)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "feature_path = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/train_features_500.pt\"\n",
    "\n",
    "loaded_data = torch.load(feature_path)\n",
    "features_tensor = loaded_data['features']\n",
    "labels_tensor = loaded_data['labels']\n",
    "\n",
    "\n",
    "print(\"불러온 피처 텐서의 모양:\", features_tensor.shape)\n",
    "print(\"불러온 라벨 텐서의 모양:\", labels_tensor.shape)\n",
    "print(features_tensor[0][0][0][1])\n",
    "print(labels_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a5019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run generation llava - val\n",
    "# model_type = \"llava1.5\"\n",
    "# cur_batch = 5\n",
    "# cur_split = \"val\"\n",
    "\n",
    "# model, proc, tok = load_model(model_type)\n",
    "# joint_loader = load_dataset(cur_split, cur_batch)\n",
    "\n",
    "# run_generation_w_logits(model, proc, tok, joint_loader, model_type, cur_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c473dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run generation qwen - val\n",
    "# model_type = \"instructblip\"\n",
    "# cur_batch = 15\n",
    "# cur_split = \"train\"\n",
    "\n",
    "# model, proc, tok = load_model(model_type)\n",
    "# joint_loader = load_dataset(cur_split, cur_batch)\n",
    "\n",
    "# run_generation_w_logits(model, proc, tok, joint_loader, model_type, cur_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216d38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a924858ba6f461daf94536a2c375dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db839266deaf4af39d2f20ee84869bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/745 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model, proc, tok \u001b[38;5;241m=\u001b[39m load_model(model_type)\n\u001b[1;32m      8\u001b[0m joint_loader \u001b[38;5;241m=\u001b[39m load_dataset(cur_split, cur_batch, start_idx)\n\u001b[0;32m---> 10\u001b[0m test\u001b[38;5;241m=\u001b[39m\u001b[43mrun_generation_w_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_split\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 61\u001b[0m, in \u001b[0;36mrun_generation_w_logits\u001b[0;34m(model, proc, tok, joint_loader, model_type, split)\u001b[0m\n\u001b[1;32m     58\u001b[0m attentions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mattentions\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# head별 entropy 계산 (마지막 layer만 예시로)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m attn_last \u001b[38;5;241m=\u001b[39m \u001b[43mattentions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m   \u001b[38;5;66;03m# (B, H, tgt_len, src_len)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m entropies \u001b[38;5;241m=\u001b[39m attention_entropy(attn_last)  \u001b[38;5;66;03m# (B, H)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 모델 generation 호출 (텍스트 생성)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# run generation instructblip - val\n",
    "model_type = \"llava\"\n",
    "cur_batch = 5\n",
    "cur_split = \"train\"\n",
    "start_idx = 11270\n",
    "\n",
    "model, proc, tok = load_model(model_type)\n",
    "\n",
    "\n",
    "joint_loader = load_dataset(cur_split, cur_batch, start_idx)\n",
    "\n",
    "test=run_generation_w_logits(model, proc, tok, joint_loader, model_type, cur_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "371aeada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'image_hidden_states'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ff65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation and scoring\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGD_TRAIN_PATH = \"/data3/KJE/code/WIL_DeepLearningProject_2/VLM_Hallu/output/llava1.5_VizWiz/logits_0916_2251.json\"\n",
    "\n",
    "with open(SGD_TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "val_data = pd.read_csv(SGD_VAL_PATH)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_eval  = pd.DataFrame(eval_data)\n",
    "\n",
    "# 사용할 단일 특징 목록\n",
    "FEATURES = [\"logit_mean\", \"logit_min\", \"logit_max\", \"logprob\"]\n",
    "TARGET   = \"hallu_label\"\n",
    "\n",
    "y_train_raw = df_train[TARGET].values\n",
    "if np.issubdtype(df_train[TARGET].dtype, np.number):\n",
    "    le = None\n",
    "    y_train = y_train_raw.astype(int)\n",
    "else:\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train_raw)\n",
    "\n",
    "# 평가용 라벨 (동일 인코더 적용)\n",
    "if le is None:\n",
    "    y_eval = df_eval[TARGET].astype(int).values\n",
    "else:\n",
    "    # 평가셋 라벨에 학습셋에 없던 클래스가 있으면 예외 발생하므로 주의\n",
    "    y_eval = le.transform(df_eval[TARGET].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logit_hallu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
